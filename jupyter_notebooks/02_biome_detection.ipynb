{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4efc30c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import axes\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import seaborn as sns\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# sns.set(rc={\"figure.dpi\":200,})\n",
    "# sns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})\n",
    "# sns.set_context('notebook')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "import random\n",
    "\n",
    "# for regression P values\n",
    "import statsmodels.api as sm\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.float_format = '{:.2f}'.format\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting maps\n",
    "import os\n",
    "os.environ[\"PROJ_LIB\"] = os.path.join(os.environ[\"CONDA_PREFIX\"], \"share\", \"proj\")\n",
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a3be3",
   "metadata": {},
   "source": [
    "# Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_values(arr):\n",
    "    '''\n",
    "    Scale an array between -1 to +1\n",
    "    '''\n",
    "    return 2.*(arr - np.min(arr))/np.ptp(arr)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_range(start, end,cell_width =10):\n",
    "    '''\n",
    "    get the ranges of the cells\n",
    "    '''\n",
    "    num_iter = (abs(start) + abs(end))/cell_width\n",
    "    range_lst = []\n",
    "    for i in range(int(num_iter)+1):\n",
    "        range_lst.append(start+i*cell_width)\n",
    "#         print(start+i*cell_width)\n",
    "    return range_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_month(df, yrmonth):\n",
    "    df['time_counter'] = df['time_counter'].astype(\"string\")\n",
    "    df = df.loc[df['time_counter'].str.contains(yrmonth, case=False)]\n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46063bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_nav_lat(df):\n",
    "    '''\n",
    "    Round up the coordinates to 2 decimal places\n",
    "    '''\n",
    "    df['nav_lat'] = df['nav_lat'].apply(lambda x:round(x,2))\n",
    "    df['nav_lon'] = df['nav_lon'].apply(lambda x:round(x,2))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eed825",
   "metadata": {},
   "source": [
    "# Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111cc4c",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f792893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def fit_random_forest_regression(grids_df_lst,feat_names,target):\n",
    "    grid_reg_score = []\n",
    "    grid_reg_coef = []\n",
    "    \n",
    "    data_count = []\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for grid_i in grids_df_lst:\n",
    "        \n",
    "        X = grid_i[feat_names].values\n",
    "        y = np.array(grid_i[target].values)\n",
    "   \n",
    "        if np.isnan(X).any():\n",
    "            print(X)\n",
    "            raise ValueError\n",
    "        \n",
    "        if (len(X) == 0) or (len(y) == 0):\n",
    "            grid_reg_score.append(None)\n",
    "            grid_reg_coef.append(None)\n",
    "        else:\n",
    "        \n",
    "            rf = RandomForestRegressor(n_estimators=150)\n",
    "            rf.fit(X, y)\n",
    "\n",
    "            grid_reg_score.append(rf.score(X, y))\n",
    "            grid_reg_coef.append(rf.feature_importances_)\n",
    "        data_count.append(len(X))\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_column_names = ['cell_id','lon_min','lon_max','lat_min','lat_max', 'data_count',\n",
    "                       'reg_score','reg_coef']        \n",
    "    save_df = pd.DataFrame(columns=df_column_names)\n",
    "    \n",
    "    nav_lat_max_lst = []\n",
    "    nav_lat_min_lst = []\n",
    "    nav_lon_max_lst = []\n",
    "    nav_lon_min_lst = []\n",
    "\n",
    "    for grid_i in grids_df_lst:\n",
    "        nav_lat_max_lst.append(grid_i['nav_lat'].max())\n",
    "        nav_lat_min_lst.append(grid_i['nav_lat'].min())\n",
    "        nav_lon_max_lst.append(grid_i['nav_lon'].max())\n",
    "        nav_lon_min_lst.append(grid_i['nav_lon'].min())\n",
    "    \n",
    "    save_df['cell_id'] = range(0, len(grids_df_lst))\n",
    "    save_df['lon_min'] = nav_lon_min_lst\n",
    "    save_df['lon_max'] = nav_lon_max_lst\n",
    "    save_df['lat_min'] = nav_lat_min_lst\n",
    "    save_df['lat_max'] = nav_lat_max_lst\n",
    "    save_df['data_count'] = data_count\n",
    "    \n",
    "    save_df['reg_score'] = grid_reg_score\n",
    "    save_df['reg_coef'] = grid_reg_coef\n",
    "    \n",
    "    print(\"Returning values: \", df_column_names)\n",
    "    return save_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a032c98",
   "metadata": {},
   "source": [
    "## MV Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec883ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "def fit_multivariate_lin_regression(grids_df_lst,feat_names,target, is_natural=True):\n",
    "    '''\n",
    "    https://satishgunjal.com/multivariate_lr_scikit/ \n",
    "    '''\n",
    "    # Fit Regression model\n",
    "    grid_reg_score = []\n",
    "    grid_reg_coef = []\n",
    "    grid_reg_intercept = []\n",
    "    \n",
    "    p_values_intercept = []\n",
    "    p_values_sst = []\n",
    "    p_values_dic = []\n",
    "    p_values_alk = []\n",
    "    \n",
    "    data_count = []\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for grid_i in grids_df_lst:\n",
    "#         feat_lst=[]\n",
    "#         for index, row in grid_i.iterrows():\n",
    "#             row_feat_lst = [row[feat] for feat in feat_names]\n",
    "#             feat_lst.append(np.array(row_feat_lst))\n",
    "        \n",
    "#         X = grid_i.values[:,2:5]\n",
    "        X = grid_i[feat_names].values\n",
    "        y = np.array(grid_i[target].values)\n",
    "   \n",
    "        if np.isnan(X).any():\n",
    "            print(X)\n",
    "            raise ValueError\n",
    "        \n",
    "        if (len(X) == 0) or (len(y) == 0) or(len(X) == 1) or (len(y) == 1) :\n",
    "            grid_reg_score.append(None)\n",
    "            grid_reg_coef.append(None)\n",
    "            grid_reg_intercept.append(None)\n",
    "            p_values_intercept.append(None)\n",
    "            p_values_sst.append(None)\n",
    "            p_values_dic.append(None)\n",
    "            p_values_alk.append(None)\n",
    "            \n",
    "            data_count.append(len(X))\n",
    "            \n",
    "        else:\n",
    "            data_count.append(len(X))\n",
    "        \n",
    "            # y = mX + c\n",
    "            lin_reg = linear_model.LinearRegression().fit(X, y)\n",
    "#             lin_reg = linear_model.Ridge(alpha=1.0).fit(X, y)\n",
    "\n",
    "            X2 = sm.add_constant(X)\n",
    "            est = sm.OLS(y, X2).fit()\n",
    "            _p_ = est.pvalues\n",
    "            # print(f\"P values are: {_p_}\")\n",
    "            if len(_p_) == 4:\n",
    "                p_values_intercept.append(_p_[0])\n",
    "                p_values_sst.append(_p_[1])\n",
    "                p_values_dic.append(_p_[2])\n",
    "                p_values_alk.append(_p_[3])\n",
    "            else:\n",
    "                p_values_intercept.append(None)\n",
    "                p_values_sst.append(None)\n",
    "                p_values_dic.append(None)\n",
    "                p_values_alk.append(None)\n",
    "            \n",
    "            grid_reg_coef.append(lin_reg.coef_) #slope m\n",
    "            grid_reg_intercept.append(lin_reg.intercept_) #intercept c\n",
    "            grid_reg_score.append(lin_reg.score(X, y)) #quality or a confidence score\n",
    "               \n",
    "            count = count + 1\n",
    "    \n",
    "    \n",
    "    df_column_names = ['cell_id','lon_min','lon_max','lat_min','lat_max', 'data_count',\n",
    "                       'reg_score','reg_coef', 'reg_intercept','p_intercept']\n",
    "    \n",
    "    for fn in feat_names:\n",
    "        df_column_names.append(f\"p_{fn}\")\n",
    "        \n",
    "    print(\"Returning values: \", df_column_names)\n",
    "        \n",
    "    save_df = pd.DataFrame(columns=df_column_names)\n",
    "    nav_lat_max_lst = []\n",
    "    nav_lat_min_lst = []\n",
    "    nav_lon_max_lst = []\n",
    "    nav_lon_min_lst = []\n",
    "\n",
    "    for grid_i in grids_df_lst:\n",
    "        nav_lat_max_lst.append(grid_i['nav_lat'].max())\n",
    "        nav_lat_min_lst.append(grid_i['nav_lat'].min())\n",
    "        nav_lon_max_lst.append(grid_i['nav_lon'].max())\n",
    "        nav_lon_min_lst.append(grid_i['nav_lon'].min())\n",
    "    \n",
    "    save_df['cell_id'] = range(0, len(grids_df_lst))\n",
    "    save_df['lon_min'] = nav_lon_min_lst\n",
    "    save_df['lon_max'] = nav_lon_max_lst\n",
    "    save_df['lat_min'] = nav_lat_min_lst\n",
    "    save_df['lat_max'] = nav_lat_max_lst\n",
    "    save_df['data_count'] = data_count\n",
    "    \n",
    "    save_df['reg_score'] = grid_reg_score\n",
    "    save_df['reg_coef'] = grid_reg_coef\n",
    "    save_df['reg_intercept'] = grid_reg_intercept\n",
    "    \n",
    "    save_df['p_intercept'] = p_values_intercept\n",
    "    save_df['p_sst'] = p_values_sst\n",
    "    if is_natural:\n",
    "        save_df['p_dicp'] = p_values_dic\n",
    "    else:\n",
    "        save_df['p_dic'] = p_values_dic\n",
    "    save_df['p_alk'] = p_values_alk\n",
    "        \n",
    "    return save_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(feat_names,target,grids_df_lst,grid_reg_score, grid_reg_coef, grid_reg_intercept,month,cell_width):\n",
    "    save_df = pd.DataFrame(columns=['cell_id','lon_min','lon_max','lat_min','lat_max',\n",
    "                                'reg_score','reg_coef', 'reg_intercept'])\n",
    "    nav_lat_max_lst = []\n",
    "    nav_lat_min_lst = []\n",
    "    nav_lon_max_lst = []\n",
    "    nav_lon_min_lst = []\n",
    "\n",
    "    for grid_i in grids_df_lst:\n",
    "        nav_lat_max_lst.append(grid_i['nav_lat'].max())\n",
    "        nav_lat_min_lst.append(grid_i['nav_lat'].min())\n",
    "        nav_lon_max_lst.append(grid_i['nav_lon'].max())\n",
    "        nav_lon_min_lst.append(grid_i['nav_lon'].min())\n",
    "    \n",
    "    save_df['cell_id'] = range(0, len(grids_df_lst))\n",
    "    save_df['lon_min'] = nav_lon_min_lst\n",
    "    save_df['lon_max'] = nav_lon_max_lst\n",
    "    save_df['lat_min'] = nav_lat_min_lst\n",
    "    save_df['lat_max'] = nav_lat_max_lst\n",
    "    save_df['reg_score'] = grid_reg_score\n",
    "    save_df['reg_coef'] = grid_reg_coef\n",
    "    save_df['reg_intercept'] = grid_reg_intercept\n",
    "    \n",
    "    feat = '_'.join(feat_names)\n",
    "    \n",
    "    filepath = f\"../csv_files/reg_model_{month}_{cell_width}_{target}_{feat}.csv\"\n",
    "    save_df.to_csv(filepath)\n",
    "    print(f\"\\n File saved at {filepath}\")\n",
    "    return save_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_grids(df_month,cell_width=2):\n",
    "    # Prepare the cells\n",
    "    nav_lat_grids = get_cell_range(start = -90, end = 90 ,cell_width = cell_width)\n",
    "    nav_lon_grids = get_cell_range(start = -180, end = 180 ,cell_width = cell_width)\n",
    "    \n",
    "    if nav_lat_grids[-1] != 90:\n",
    "        nav_lat_grids.append(90)\n",
    "        \n",
    "    if nav_lon_grids[-1] != 180:\n",
    "        nav_lon_grids.append(180)\n",
    "        \n",
    "    # Build the grids. Store in a list.\n",
    "    grids_df_lst=[]\n",
    "    for lat_i in range(len(nav_lat_grids)):\n",
    "        for lon_j in range(len(nav_lon_grids)):\n",
    "            if((nav_lat_grids[lat_i] == 90) or (nav_lon_grids[lon_j] == 180)):\n",
    "                break\n",
    "            elif ((lat_i == len(nav_lat_grids) - 1) or (lon_j == len(nav_lon_grids) - 1)):\n",
    "                break\n",
    "            else:\n",
    "                _df_ = df_month.loc[\n",
    "                    (df_month['nav_lat'] >= nav_lat_grids[lat_i]) & \n",
    "                    (df_month['nav_lat'] <  nav_lat_grids[lat_i+1]) &\n",
    "                    (df_month['nav_lon'] >= nav_lon_grids[lon_j]) & \n",
    "                    (df_month['nav_lon'] <  nav_lon_grids[lon_j+1])\n",
    "                                ]\n",
    "                grids_df_lst.append(_df_)\n",
    "    \n",
    "    print(f\"\\n Total no. of generated cells: {len(grids_df_lst)}\")\n",
    "    \n",
    "    return grids_df_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c53b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_df, feat_names, target, cell_width, is_natural):\n",
    "    # build grid cells\n",
    "    grids_df_lst = build_grids(model_df,cell_width)\n",
    "    # fit linear regression\n",
    "    save_df = fit_multivariate_lin_regression(grids_df_lst,feat_names,target, is_natural)   \n",
    "    return save_df, grids_df_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a8d72",
   "metadata": {},
   "source": [
    "# Run MVLR in 2x2 boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "months_12 = {\n",
    "            'jan':'-01-',\n",
    "            'feb':'-02-',\n",
    "            'mar':'-03-',\n",
    "            'apr':'-04-',\n",
    "            'may':'-05-',\n",
    "            'jun':'-06-',\n",
    "            'jul':'-07-',\n",
    "            'aug':'-08-',\n",
    "            'sep':'-09-',\n",
    "            'oct':'-10-',\n",
    "            'nov':'-11-',\n",
    "            'dec':'-12-',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba23da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# _month = 'jan'\n",
    "\n",
    "months = list(months_12.keys())\n",
    "cell_width = 2\n",
    "\n",
    "## function to set up dataframe for Hierarchical clustering\n",
    "fn_sst = lambda row: row['reg_coef'][0]\n",
    "fn_dic = lambda row: row['reg_coef'][1]\n",
    "fn_alk = lambda row: row['reg_coef'][2]\n",
    "\n",
    "for yr in range(1958, 1980):\n",
    "    for mnth in months_12:\n",
    "\n",
    "        print(yr, mnth)\n",
    "        data_df = pd.read_pickle(f\"../carbon_data_preprocessed/ocean_data_{yr}_df.pkl\")\n",
    "    \n",
    "        if 'sosstsst' in data_df.columns:\n",
    "            data_df = data_df.rename(columns={'sosstsst': 'SST'})\n",
    "        \n",
    "        ## Remove known outliers\n",
    "        data_df = data_df.loc[data_df['DICP'] >= 1500]\n",
    "        # data_df = data_df.loc[data_df['DIC'] >= 1500]\n",
    "        # Select ALK above 1700\n",
    "        data_df = data_df.loc[data_df['ALK'] >= 1700]\n",
    "        # Select SST below 40\n",
    "        data_df = data_df.loc[data_df['SST'] <= 40]\n",
    "        # Select fco2 below 500\n",
    "        data_df = data_df.loc[data_df['fco2_pre'] <= 500]\n",
    "    \n",
    "        data_df = round_nav_lat(data_df)\n",
    "        df_month = get_year_month(df = data_df , yrmonth = months_12[mnth])\n",
    "        df_month['area'] = df_month['e1t'] * df_month['e2t']\n",
    "    \n",
    "        print(df_month.columns)\n",
    "    \n",
    "        print(\"Gridding 2x2\")\n",
    "        grids_df_lst = build_grids(df_month,cell_width=cell_width)\n",
    "    \n",
    "        print(\"Running MVLR\")\n",
    "        reg_df_mvlr = fit_multivariate_lin_regression(grids_df_lst = grids_df_lst ,\n",
    "                                        feat_names=['SST','DICP', 'ALK'],target='fco2_pre')\n",
    "    \n",
    "        ## Remove grids with Zero number of coordinates\n",
    "        reg_df_mvlr = reg_df_mvlr[reg_df_mvlr.data_count != 0]\n",
    "        reg_df_mvlr = reg_df_mvlr[reg_df_mvlr.data_count != 1]\n",
    "        \n",
    "        ## Regression above 96% of significance\n",
    "        reg_df_p = reg_df_mvlr[(reg_df_mvlr.p_sst <= 0.04) & (reg_df_mvlr.p_dicp <= 0.04)& (reg_df_mvlr.p_alk <= 0.04)]\n",
    "    \n",
    "        ## Saving the slopes\n",
    "        hc_df = pd.DataFrame()\n",
    "        hc_df['cell_id'] = reg_df_p['cell_id']\n",
    "        hc_df['slope_sst'] = reg_df_p.apply(fn_sst,axis=1)\n",
    "        hc_df['slope_dicp'] = reg_df_p.apply(fn_dic,axis=1)\n",
    "        hc_df['slope_alk'] = reg_df_p.apply(fn_alk,axis=1)\n",
    "    \n",
    "        hc_df['slope_sst_std'] = (hc_df['slope_sst']-hc_df['slope_sst'].mean())/hc_df['slope_sst'].std()\n",
    "        hc_df['slope_dicp_std'] = (hc_df['slope_dicp']-hc_df['slope_dicp'].mean())/hc_df['slope_dicp'].std()\n",
    "        hc_df['slope_alk_std'] = (hc_df['slope_alk']-hc_df['slope_alk'].mean())/hc_df['slope_alk'].std()\n",
    "    \n",
    "        appended_data = []\n",
    "        for index, row in hc_df.iterrows():\n",
    "            # get the corresponding grid from the list of grids\n",
    "            _df_ = grids_df_lst[index]\n",
    "            _df_['grid_id'] = index\n",
    "            _df_['slope_sst'] = row['slope_sst']\n",
    "            _df_['slope_dicp'] = row['slope_dicp']\n",
    "            _df_['slope_alk'] = row['slope_alk']\n",
    "            _df_['slope_sst_std'] = row['slope_sst_std']\n",
    "            _df_['slope_dicp_std'] = row['slope_dicp_std']\n",
    "            _df_['slope_alk_std'] = row['slope_alk_std']\n",
    "            appended_data.append(_df_)\n",
    "            \n",
    "        appended_data = pd.concat(appended_data)\n",
    "        appended_data.to_pickle(f\"output_files/spatial_regression_{yr}_{mnth}.pkl\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d14cb8",
   "metadata": {},
   "source": [
    "# Biome detection- Jan, 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacef9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "_year = 2009\n",
    "_month = 'jan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b5e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_pickle(f\"output_files/spatial_regression_{_year}_{_month}.pkl\")\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3b33d",
   "metadata": {},
   "source": [
    "# Adaptive HC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c0f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df = data_df.groupby('cell_id').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b22cd",
   "metadata": {},
   "source": [
    "# Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hc_df[['slope_sst_std', 'slope_dicp_std', 'slope_alk_std']].values\n",
    "Z = hierarchy.linkage(X, method='ward')\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97661244",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f937a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchy.fcluster(Z, t=100, criterion='distance')\n",
    "# hierarchy.dendrogram(Z, color_threshold=0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718d96c",
   "metadata": {},
   "source": [
    "## Automtic Cut Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d6f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_children = len(norm_hc_df)\n",
    "tot_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat_arr(dendro_dict_lst, index_, arr):\n",
    "    feat_stack = np.array([])\n",
    "    for item in dendro_dict_lst:\n",
    "            if item['id'] == index_:\n",
    "                feat_stack = item['feat_stack']\n",
    "                break\n",
    "    if feat_stack.size == 0:\n",
    "        feat_stack = [arr[index_]]\n",
    "    return feat_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_index_list(dendro_dict_lst, index_, df):\n",
    "    _l_ = []\n",
    "    for item in dendro_dict_lst:\n",
    "            if item['id'] == index_:\n",
    "                _l_ = item['grid_index']\n",
    "                break\n",
    "    if len(_l_) == 0:\n",
    "        _l_ = [df.index[index_]]\n",
    "    return _l_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/73103010/matching-up-the-output-of-scipy-linkage-and-dendrogram\n",
    "\n",
    "def append_index(n, i, cluster_id_list):\n",
    "    # refer to the recursive progress in\n",
    "    # https://github.com/scipy/scipy/blob/4cf21e753cf937d1c6c2d2a0e372fbc1dbbeea81/scipy/cluster/hierarchy.py#L3549\n",
    "\n",
    "    # i is the idx of cluster(counting in all 2 * n - 1 clusters)\n",
    "    # so i-n is the idx in the \"Z\"\n",
    "    if i < n:\n",
    "        return\n",
    "    aa = int(Z[i - n, 0])\n",
    "    ab = int(Z[i - n, 1])\n",
    "\n",
    "    append_index(n, aa, cluster_id_list)\n",
    "    append_index(n, ab, cluster_id_list)\n",
    "\n",
    "    cluster_id_list.append(i-n)\n",
    "    # Imitate the progress in hierarchy.dendrogram\n",
    "    # so how `i-n` is appended , is the same as how the element in 'icoord'&'dcoord' be.\n",
    "    return\n",
    "\n",
    "def get_linkid_clusterid_relation(Z):\n",
    "    Zs = Z.shape\n",
    "    n = Zs[0] + 1\n",
    "    i = 2 * n - 2\n",
    "    cluster_id_list = []\n",
    "    append_index(n, i, cluster_id_list)\n",
    "    # cluster_id_list[i] is the cluster idx(in Z) that the R['icoord'][i]/R['dcoord'][i] corresponds to\n",
    "\n",
    "    dict_linkid_2_clusterid = {linkid: clusterid for linkid, clusterid in enumerate(cluster_id_list)}\n",
    "    dict_clusterid_2_linkid = {clusterid: linkid for linkid, clusterid in enumerate(cluster_id_list)}\n",
    "    return dict_linkid_2_clusterid, dict_clusterid_2_linkid\n",
    "\n",
    "dict_linkid_2_clusterid, dict_clusterid_2_linkid = get_linkid_clusterid_relation(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa734df",
   "metadata": {},
   "source": [
    "## Build Dendro dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clust_num = tot_children\n",
    "print(f'Start Cluster number = {clust_num}')\n",
    "dendro_dict_lst = []\n",
    "# index_count = len(Z) - 1\n",
    "index_count = 0\n",
    "for row in Z:\n",
    "    dendro_dict ={}\n",
    "    dendro_dict['id'] = clust_num    \n",
    "    dendro_dict['info'] = row\n",
    "    dendro_dict['feat_stack'] = np.array([])\n",
    "    dendro_dict['variance'] = np.array([])\n",
    "    dendro_dict['Z_index'] = index_count\n",
    "    dendro_dict['grid_index'] = []\n",
    "\n",
    "    dendro_dict_lst.append(dendro_dict)\n",
    "    clust_num = clust_num + 1    \n",
    "    index_count = index_count + 1\n",
    "\n",
    "print(f'End Cluster number = {clust_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "count = 0\n",
    "for dendro in dendro_dict_lst:\n",
    "    arr = dendro['info']\n",
    "    first_index = int(arr[0])\n",
    "    second_index = int(arr[1])\n",
    "    child_num = arr[3]\n",
    "    if child_num == 2:\n",
    "        dendro['feat_stack'] =np.append([X[first_index]],[X[second_index]],axis = 0)\n",
    "        dendro['variance'] = np.var(dendro['feat_stack'])\n",
    "        _l_ = [norm_hc_df.index[first_index]]\n",
    "        _l_.append(norm_hc_df.index[second_index])\n",
    "        dendro['grid_index'] = _l_\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "count = 0\n",
    "\n",
    "for dendro in dendro_dict_lst:\n",
    "    arr_ = dendro['info']\n",
    "    first_index = int(arr_[0])\n",
    "    second_index = int(arr_[1])\n",
    "    child_num = arr_[3]\n",
    "    if child_num > 2:\n",
    "#         print(arr[3])\n",
    "#         print(first_index)\n",
    "#         print(second_index)\n",
    "        left_index = get_feat_arr(dendro_dict_lst = dendro_dict_lst, \n",
    "                         index_ = first_index, \n",
    "                         arr = X)\n",
    "        \n",
    "        right_index = get_feat_arr(dendro_dict_lst = dendro_dict_lst, \n",
    "                         index_ = second_index, \n",
    "                         arr = X)  \n",
    "        dendro['feat_stack'] =np.append(left_index,right_index,axis = 0)\n",
    "        dendro['variance'] = np.var(dendro['feat_stack'])\n",
    "        \n",
    "        left_grid_index = get_grid_index_list(dendro_dict_lst = dendro_dict_lst,\n",
    "                                              index_ = first_index, df = norm_hc_df)\n",
    "        right_grid_index = get_grid_index_list(dendro_dict_lst = dendro_dict_lst,\n",
    "                                               index_ = second_index, df = norm_hc_df)\n",
    "        # appending lists to one list\n",
    "        dendro['grid_index'] = [*left_grid_index, *right_grid_index]\n",
    "\n",
    "\n",
    "        count = count + 1    \n",
    "#         break\n",
    "# print(dendro)\n",
    "print(count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec3fcf",
   "metadata": {},
   "source": [
    "## Run over multiple set of del_var and del_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6529a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_through_dendrogram(delta_var, delta_dist, dendro_dict_lst):\n",
    "\n",
    "    process_left = []\n",
    "    process_right = []\n",
    "\n",
    "    cluster_list = []\n",
    "    z_index_list = []\n",
    "\n",
    "    # get the top most element from the dendrogram and set is as \"Start\".\n",
    "    start = dendro_dict_lst[-1]\n",
    "\n",
    "    head = start['id']\n",
    "    current_left = start['info'][0]\n",
    "    current_right = start['info'][1]\n",
    "    current_height = start['info'][2]\n",
    "    current_variance = start['variance']\n",
    "    current_z_index = start['Z_index']\n",
    "\n",
    "    while(head):\n",
    "        left_height = None\n",
    "        left_variance = None\n",
    "        left_z_index = None\n",
    "        right_height = None\n",
    "        right_variance = None\n",
    "        right_z_index = None\n",
    "\n",
    "        # left_child_num = 1\n",
    "        # right_child_num = 1\n",
    "\n",
    "        for item in dendro_dict_lst:\n",
    "            if item['id'] == current_left:\n",
    "                left_height = item['info'][2]\n",
    "                left_variance = item['variance']\n",
    "                left_z_index = item['Z_index']\n",
    "\n",
    "        for item in dendro_dict_lst:\n",
    "            if item['id'] == current_right:\n",
    "                right_height = item['info'][2]\n",
    "                right_variance = item['variance']\n",
    "                right_z_index = item['Z_index']\n",
    "\n",
    "        if right_height:\n",
    "            change_height_right = abs(current_height - right_height)\n",
    "            change_variance_right = abs(current_variance - right_variance)\n",
    "            if ((change_height_right > delta_dist) and (change_variance_right > delta_var)):\n",
    "                process_right.append(current_right)\n",
    "            else:\n",
    "                cluster_list.append(current_right)\n",
    "                z_index_list.append(right_z_index)\n",
    "\n",
    "        else:\n",
    "            cluster_list.append(current_right)\n",
    "    #         z_index_list.append(current_right)\n",
    "\n",
    "        if left_height:\n",
    "            change_height_left = abs(current_height - left_height)\n",
    "            change_variance_left = abs(current_variance - left_variance)\n",
    "\n",
    "            if ((change_height_left > delta_dist) and (change_variance_left > delta_var)): \n",
    "                #change_variance_left = np.inf\n",
    "                    process_left.append(current_left)\n",
    "            else:\n",
    "                    # splitting the current head\n",
    "                    cluster_list.append(current_left)\n",
    "                    z_index_list.append(left_z_index)\n",
    "        else:\n",
    "            cluster_list.append(current_left)\n",
    "    #         z_index_list.append(current_left)\n",
    "\n",
    "        ## Reassign head\n",
    "        if process_left:\n",
    "            head = process_left.pop()\n",
    "        elif process_right:\n",
    "            head = process_right.pop()\n",
    "        else:\n",
    "            head = None\n",
    "\n",
    "        ## Reassign\n",
    "        if head:\n",
    "            for item in dendro_dict_lst:\n",
    "                if item['id'] == head:\n",
    "                    current_left = item['info'][0]\n",
    "                    current_right = item['info'][1]\n",
    "                    current_height = item['info'][2]\n",
    "                    current_variance = item['variance']\n",
    "                    current_z_index = item['Z_index']\n",
    "                    \n",
    "    return cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_clusters(cluster_list):\n",
    "    heights_lst = []\n",
    "    grid_index_list = []\n",
    "    for c in cluster_list:\n",
    "        found = False\n",
    "        for item in dendro_dict_lst:\n",
    "                if item['id'] == c:\n",
    "    #                 print(item['id'])\n",
    "                    found = True\n",
    "                    ht = item['info'][2]\n",
    "                    heights_lst.append(ht)\n",
    "                    grid_index_list.append(item['grid_index'])\n",
    "        if not found:\n",
    "#             print(f\"!!! Singleton cluster at {c}\")\n",
    "            heights_lst.append(0.0)\n",
    "            grid_index_list.append([norm_hc_df.index[int(c)]])\n",
    "    \n",
    "    ## Extract clusters\n",
    "    cluster_lbl = 1\n",
    "    l__indices = []\n",
    "    l__lbls = []\n",
    "    for grid_list in grid_index_list:\n",
    "        if type(grid_list) != list:\n",
    "            l__indices.append(grid_list)\n",
    "            l__lbls.append(cluster_lbl)\n",
    "        else:\n",
    "            for i in grid_list:\n",
    "                # print(f'going wrong: ', i)\n",
    "                l__indices.append(i)\n",
    "                l__lbls.append(cluster_lbl)\n",
    "        cluster_lbl = cluster_lbl + 1\n",
    "\n",
    "    _df_ = pd.DataFrame(l__lbls, index =l__indices,columns =['cluster'])\n",
    "    cluster_df = pd.merge(norm_hc_df, _df_, left_index=True, right_index=True)\n",
    "    return cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc140fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bic_aic_score(cluster_df):\n",
    "    #size of data set -> N datapoints with d no. of features\n",
    "    N, d = X.shape\n",
    "    #unique labels\n",
    "    labels_unique = cluster_df['cluster'].unique()\n",
    "\n",
    "    loglikelihood = 0  \n",
    "    for lbl in labels_unique:\n",
    "        df_lbl = cluster_df.loc[cluster_df['cluster'] == lbl]\n",
    "        loglikelihood =loglikelihood + np.log(len(df_lbl)/N)\n",
    "\n",
    "    #BIC\n",
    "    BIC = -2 * loglikelihood + d * np.log(N)\n",
    "    AIC = -2 * loglikelihood + 2*d\n",
    "    return BIC, AIC, labels_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f70323c",
   "metadata": {},
   "source": [
    "### Set distance variance list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_variance_list = [0.1,0.5, 0.8, 1.0]\n",
    "del_distance_list = [20,25,28,30,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b00529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "plot_del_var = []\n",
    "plot_del_dist = []\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "tot_clusters = []\n",
    "\n",
    "for del_dist in del_distance_list:\n",
    "    for del_var in del_variance_list:\n",
    "#         print(del_var, del_dist)\n",
    "        cluster_list = run_through_dendrogram(del_var, del_dist, dendro_dict_lst)\n",
    "        cluster_df = get_all_clusters(cluster_list)\n",
    "        BIC, AIC, labels_unique = get_bic_aic_score(cluster_df)\n",
    "        \n",
    "        # Add to the lists\n",
    "        plot_del_var.append(del_var)\n",
    "        plot_del_dist.append(del_dist)\n",
    "        bic_scores.append(BIC)\n",
    "        aic_scores.append(AIC)\n",
    "        tot_clusters.append(len(labels_unique))\n",
    "        \n",
    "#         if ((del_var == 0.5) and (del_dist == 15.0)):\n",
    "#             print('hello', len(labels_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18816a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(plot_del_dist, plot_del_var, bic_scores, tot_clusters )),\n",
    "               columns =['Del_Dist', 'Del_Var', 'BIC', 'Clusters'])\n",
    "df = df.round(2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33151fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_latex(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df.pivot('Del_Dist', 'Del_Var', 'BIC')\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_pivot, annot=True, fmt=\".2f\", cmap=\"summer_r\", vmin=40, vmax=80, annot_kws={\"fontsize\":12})\n",
    "plt.xlabel(\" \")\n",
    "plt.ylabel(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df.pivot('Del_Dist', 'Del_Var', 'Clusters')\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbc2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_pivot, annot=True, cmap=\"summer_r\", vmin=3, vmax=10, annot_kws={\"fontsize\":12})\n",
    "plt.xlabel(\" \")\n",
    "plt.ylabel(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f3d44",
   "metadata": {},
   "source": [
    "### BIC Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb87b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.groupby(['Del_Dist'], as_index=False)['BIC', 'Clusters'].mean()\n",
    "df_ = df_.astype({\"Clusters\":'int'})\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef633a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_['Del_Dist'].values\n",
    "data1 = df_['BIC'].values\n",
    "data2 = df_['Clusters']\n",
    "\n",
    "# data2 = np.array(data2)\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Change in distance', fontsize=15)\n",
    "ax1.set_ylabel('Mean BIC', color=color, fontsize=15)\n",
    "ax1.plot(t, data1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:brown'\n",
    "ax2.set_ylabel('Mean #Cluster', color=color, fontsize=15)  # we already handled the x-label with ax1\n",
    "ax2.scatter(t, data2, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "# ax2.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42696504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.groupby(['Del_Var'], as_index=False)['BIC', 'Clusters'].mean()\n",
    "\n",
    "t = df_['Del_Var'].values\n",
    "data1 = df_['BIC'].values\n",
    "data2 = df_['Clusters'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0180e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.array(data2).astype(int).tolist()\n",
    "\n",
    "print(data2)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Change in variance', fontsize=15)\n",
    "ax1.set_ylabel('Mean BIC', color=color, fontsize=15)\n",
    "ax1.plot(t, data1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:brown'\n",
    "ax2.set_ylabel('Mean #Cluster', color=color, fontsize=15)  # we already handled the x-label with ax1\n",
    "ax2.scatter(t, data2, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30abc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bic_cluster_num_fig = px.scatter(df, x=\"Del_Dist\", y=\"Del_Var\",size='Clusters',\n",
    "#                                 color= 'BIC', hover_data=['Clusters', 'BIC'],)\n",
    "# bic_cluster_num_fig.update_layout(title = 'BIC Score and total no. of clusters generated wrt change in variance and distance',)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13221b4",
   "metadata": {},
   "source": [
    "# Distance - Variance Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df1c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "## For Jan 2009\n",
    "delta_dist = 25.0\n",
    "delta_var = 0.1\n",
    "\n",
    "process_left = []\n",
    "process_right = []\n",
    "\n",
    "cluster_list = []\n",
    "z_index_list = []\n",
    "\n",
    "# get the top most element from the dendrogram and set is as \"Start\".\n",
    "start = dendro_dict_lst[-1]\n",
    "\n",
    "head = start['id']\n",
    "current_left = start['info'][0]\n",
    "current_right = start['info'][1]\n",
    "current_height = start['info'][2]\n",
    "current_variance = start['variance']\n",
    "current_z_index = start['Z_index']\n",
    "\n",
    "test_count = 0\n",
    "\n",
    "count_stop = 0\n",
    "\n",
    "while(head):\n",
    "    print()\n",
    "    print('New head.')\n",
    "    left_height = None\n",
    "    left_variance = None\n",
    "    left_z_index = None\n",
    "    right_height = None\n",
    "    right_variance = None\n",
    "    right_z_index = None\n",
    "    \n",
    "    # left_child_num = 1\n",
    "    # right_child_num = 1\n",
    "\n",
    "    for item in dendro_dict_lst:\n",
    "        if item['id'] == current_left:\n",
    "            left_height = item['info'][2]\n",
    "            left_variance = item['variance']\n",
    "            left_z_index = item['Z_index']\n",
    "\n",
    "    for item in dendro_dict_lst:\n",
    "        if item['id'] == current_right:\n",
    "            right_height = item['info'][2]\n",
    "            right_variance = item['variance']\n",
    "            right_z_index = item['Z_index']\n",
    "        \n",
    "\n",
    "    if left_height:\n",
    "        change_height_left = abs(current_height - left_height)\n",
    "        change_variance_left = abs(current_variance - left_variance)\n",
    "        \n",
    "        print(\"Change in height:\", change_height_left)\n",
    "        print(\"Change in variance:\", change_variance_left)\n",
    "\n",
    "        if ((change_height_left > delta_dist) and (change_variance_left > delta_var)): \n",
    "            #change_variance_left = np.inf\n",
    "            print('went down.')\n",
    "            process_left.append(current_left)\n",
    "        else:\n",
    "            \n",
    "            # splitting the current head\n",
    "            cluster_list.append(current_left)\n",
    "            z_index_list.append(left_z_index)\n",
    "            count_stop = count_stop + 1\n",
    "            print(f'stopped. - {count_stop}')\n",
    "            \n",
    "    else:\n",
    "        cluster_list.append(current_left)\n",
    "        z_index_list.append(current_left)\n",
    "        count_stop = count_stop + 1\n",
    "        print(f'stopped. - {count_stop}')\n",
    "        \n",
    "    if right_height:\n",
    "        change_height_right = abs(current_height - right_height)\n",
    "        change_variance_right = abs(current_variance - right_variance)\n",
    "        \n",
    "        print(\"Change in height:\", change_height_right)\n",
    "        print(\"Change in variance:\", change_variance_right)\n",
    "\n",
    "        if ((change_height_right > delta_dist) and (change_variance_right > delta_var)):\n",
    "            print('went down.')\n",
    "            process_right.append(current_right)\n",
    "        else:\n",
    "            cluster_list.append(current_right)\n",
    "            z_index_list.append(right_z_index)\n",
    "            count_stop = count_stop + 1\n",
    "            print(f'stopped. - {count_stop}')\n",
    "            \n",
    "\n",
    "    else:\n",
    "        cluster_list.append(current_right)\n",
    "        z_index_list.append(current_right)\n",
    "        count_stop = count_stop + 1\n",
    "        print(f'stopped. - {count_stop}')\n",
    "        \n",
    "    ## Reassign head\n",
    "    if process_left:\n",
    "        head = process_left.pop()\n",
    "    elif process_right:\n",
    "        head = process_right.pop()\n",
    "    else:\n",
    "        head = None\n",
    "    \n",
    "    ## Reassign\n",
    "    if head:\n",
    "        for item in dendro_dict_lst:\n",
    "            if item['id'] == head:\n",
    "                current_left = item['info'][0]\n",
    "                current_right = item['info'][1]\n",
    "                current_height = item['info'][2]\n",
    "                current_variance = item['variance']\n",
    "                current_z_index = item['Z_index']\n",
    "\n",
    "\n",
    "        \n",
    "#     test_count = test_count + 1\n",
    "#     if test_count == 100:\n",
    "#         break\n",
    "cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee873e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "heights_lst = []\n",
    "grid_index_list = []\n",
    "for c in cluster_list:\n",
    "    found = False\n",
    "    for item in dendro_dict_lst:\n",
    "            if item['id'] == int(c):\n",
    "#                 print(item['id'])\n",
    "                found = True\n",
    "                ht = item['info'][2]\n",
    "                heights_lst.append(ht)\n",
    "                grid_index_list.append(item['grid_index'])\n",
    "    if not found:\n",
    "        print(\"singleton found\")\n",
    "        print(c)\n",
    "        heights_lst.append(0.0)\n",
    "        grid_index_list.append([norm_hc_df.index[int(c)]])\n",
    "len(grid_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7495be",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(z_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56408023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(grid_index_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd02c5",
   "metadata": {},
   "source": [
    "# Plot Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675c1f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.figure(figsize=(10, 8), dpi=300).set_facecolor('white')\n",
    "# plt.title(f\"Dendrograms at delta_var = {round(delta_var,2)}, delta_dist = {round(delta_dist,2)}, #Clusters = {len(heights_lst)}\",\n",
    "#           fontsize=16)\n",
    "\n",
    "dend = hierarchy.dendrogram(Z, color_threshold=0, above_threshold_color='k')\n",
    "LBL_COUNT = 1\n",
    "\n",
    "for z, h in zip(z_index_list, heights_lst):\n",
    "    link_id = dict_clusterid_2_linkid[z]\n",
    "    i = dend['icoord'][link_id]\n",
    "    x = 0.5 * sum(i[1:3])\n",
    "    y = h\n",
    "    plt.plot(x, y, 'ro',c='red', markersize=8) #label = f'height = {y}'\n",
    "#     plt.plot(x, y, 'ro',c=cluster_color_dict[LBL_COUNT], markersize=8) #label = f'height = {y}'\n",
    "    \n",
    "#     label = regime_names_dict[LBL_COUNT]\n",
    "    plt.annotate(LBL_COUNT, # this is the text\n",
    "                (x,y), # these are the coordinates to position the text\n",
    "                textcoords=\"offset points\", # how to position the text\n",
    "                xytext=(5,5), # distance from text to points (x,y)\n",
    "                ha='left', # horizontal alignment can be left, right or center\n",
    "                color='red',\n",
    "#                 color=cluster_color_dict[LBL_COUNT],\n",
    "                fontsize=18, weight='bold',)\n",
    "#     plt.axhline(y=h, color='black', linestyle='--', linewidth = 0.5, label = f'{LBL_COUNT} -> {h}')  \n",
    "    \n",
    "    LBL_COUNT = LBL_COUNT + 1\n",
    "    \n",
    "# # for height in heights_lst:\n",
    "# #     plt.axhline(y=height, color='black', linestyle='--', label = f'{height}')  \n",
    "# # plt.legend()\n",
    "\n",
    "plt.xlabel(\"Individual grid cells containing RCs of SST, DIC, and ALK\", fontsize=16)\n",
    "plt.ylabel(\"Euclidean Distance\", fontsize =16)\n",
    "plt.xticks([], []) # Remove X axis ticks / grid cell ids\n",
    "import matplotlib as mpl\n",
    "# mpl.rcParams['xtick.labelsize'] = 16 \n",
    "mpl.rcParams['ytick.labelsize'] = 16\n",
    "plt.savefig(f\"output_tracking/Dendrogram_{_year}_{_month}_{len(heights_lst)}_clusters\",\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea00b1c",
   "metadata": {},
   "source": [
    "## Extract Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5094a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2896b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_names_dict = {\n",
    "    1: 'ICE I',\n",
    "    2: 'ICE II',\n",
    "    3: 'SUBTR I',\n",
    "    4: 'SUBTR II',\n",
    "    6: 'SUBP + UP I',\n",
    "    7: 'SUBP + UP II',\n",
    "    5: 'SUBP + UP III',\n",
    "}\n",
    "len(regime_names_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646955b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the cluster_color_dict\n",
    "cluster_color_dict = {\n",
    "    1: sns.color_palette(\"light:blue\", as_cmap=False, n_colors=10)[3],\n",
    "    2: 'blue',\n",
    "    3: sns.color_palette(\"autumn\", as_cmap=False, n_colors=5)[4],#'yellow',\n",
    "    4: 'orange',\n",
    "    6: 'gray',\n",
    "    7: sns.color_palette(\"light:green\", as_cmap=False, n_colors=5)[2],\n",
    "    5: 'green', \n",
    "}\n",
    "\n",
    "# Define the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 2), dpi=1200)\n",
    "\n",
    "# Set the limits of the x and y axes\n",
    "ax.set_xlim(0, len(cluster_color_dict) * 2)\n",
    "ax.set_ylim(0, 2)\n",
    "\n",
    "# Iterate over the cluster_color_dict to plot the colored boxes\n",
    "for i, (cluster, color) in enumerate(cluster_color_dict.items()):\n",
    "    # Plot the colored box\n",
    "    ax.add_patch(plt.Rectangle((i * 1.1,1), 1, 0.5, color=color)) # First 1 is for padding between the colors\n",
    "    # Plot the number underneath the box\n",
    "#     ax.text(i * 2 + 0.5, -0.5, str(cluster), ha='center', va='center')\n",
    "\n",
    "# Hide the axes\n",
    "ax.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767a85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_lbl = 1\n",
    "# cluster_lbl = 0\n",
    "l__indices = []\n",
    "l__lbls = []\n",
    "for grid_list in grid_index_list:\n",
    "    if type(grid_list) != list:\n",
    "        l__indices.append(grid_list)\n",
    "        l__lbls.append(cluster_lbl)\n",
    "    else:\n",
    "        for i in grid_list:\n",
    "            # print(f'going wrong: ', i)\n",
    "            l__indices.append(i)\n",
    "            l__lbls.append(cluster_lbl)\n",
    "    cluster_lbl = cluster_lbl + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_ = pd.DataFrame(l__lbls, index =l__indices,columns =['cluster'])\n",
    "hc_df = pd.merge(norm_hc_df, _df_, left_index=True, right_index=True)\n",
    "hc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ebe2a",
   "metadata": {},
   "source": [
    "## BIC Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb96e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_all = hc_df['cluster'].values #all labels\n",
    "#size of data set -> N datapoints with d no. of features\n",
    "N, d = X.shape\n",
    "#unique labels\n",
    "labels_unique = hc_df['cluster'].unique()\n",
    "\n",
    "loglikelihood = 0  \n",
    "for lbl in labels_unique:\n",
    "    _df_ = hc_df.loc[hc_df['cluster'] == lbl]\n",
    "    loglikelihood =loglikelihood + np.log(len(_df_)/N)\n",
    "\n",
    "#BIC\n",
    "BIC = -2 * loglikelihood + d * np.log(N)\n",
    "AIC = -2 * loglikelihood + 2*d\n",
    "b_score = BIC\n",
    "b_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671178de",
   "metadata": {},
   "source": [
    "## Save clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2dec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data = []\n",
    "for index, row in hc_df.iterrows():\n",
    "    # get the corresponding grid from the list of grids\n",
    "    _df_ = grids_df_lst[index]\n",
    "    _df_['cell_id'] = index\n",
    "    _df_['cluster'] = row['cluster']\n",
    "    _df_['slope_sst'] = row['slope_sst']\n",
    "    _df_['slope_sst_std'] = row['slope_sst_std']\n",
    "    if is_natural:\n",
    "        _df_['slope_dicp'] = row['slope_dicp']\n",
    "        _df_['slope_dicp_std'] = row['slope_dicp_std']\n",
    "    else:\n",
    "        _df_['slope_dic'] = row['slope_dic']\n",
    "#         _df_['slope_dic_std'] = row['slope_dic_std']\n",
    "        \n",
    "    _df_['slope_alk'] = row['slope_alk']\n",
    "    _df_['slope_alk_std'] = row['slope_alk_std']\n",
    "#     _df_['slope_sal'] = row['slope_sal']\n",
    "#     _df_['slope_sal_std'] = row['slope_sal_std']\n",
    "    _df_['BIC'] = BIC\n",
    "    _df_['delta_var'] = delta_var\n",
    "    _df_['delta_dist'] = delta_dist\n",
    "    appended_data.append(_df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data = pd.concat(appended_data)\n",
    "appended_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f750b7e8",
   "metadata": {},
   "source": [
    "## Project on Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a75df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(14, 10), edgecolor='w')\n",
    "regime_map = Basemap(projection='cyl', resolution='c',\n",
    "            llcrnrlat=-90, urcrnrlat=90,\n",
    "            llcrnrlon=-180, urcrnrlon=180, )\n",
    "## Draw the coast.\n",
    "# regime_map.drawcoastlines()\n",
    "\n",
    "## Draw the land shades.\n",
    "# regime_map.shadedrelief()\n",
    "\n",
    "## Fill the land mass and lakes\n",
    "regime_map.fillcontinents(color='black') #color_lake='aqua'\n",
    "\n",
    "## draw parallels and meridians.\n",
    "# regime_map.drawparallels(np.arange(-90,91,10),labels=[1,1,0,1], fontsize=12)\n",
    "# regime_map.drawmeridians(np.arange(-180,181,10),labels=[1,1,0,1], rotation=45, fontsize=12)\n",
    "\n",
    "##color the sea/oceans\n",
    "# regime_map.drawmapboundary(fill_color='aqua')\n",
    "\n",
    "count = 0\n",
    "arr = np.sort((appended_data['cluster'].unique()))\n",
    "\n",
    "for cluster_num in arr:\n",
    "    _df_ = appended_data.loc[appended_data['cluster'] == cluster_num]\n",
    "    lbl = int(cluster_num)\n",
    "#     lbl = f\"{int(cluster_num)} --> {len(_df_)} ({round(_df_['area'].sum()/1000000,2)} km sq)\"\n",
    "    regime_map.scatter(_df_['nav_lon'], _df_['nav_lat'], \n",
    "                       latlon=True,marker='.', \n",
    "#                        color=cluster_color_dict[cluster_num],\n",
    "                       color=clrs_original[count],\n",
    "                       label = lbl,\n",
    "                       s=5)\n",
    "    count = count + 1\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "# plt.legend(by_label.values(), by_label.keys())\n",
    "lgnd = plt.legend(by_label.values(), by_label.keys(),loc='lower center', ncol=8, fontsize=17,\n",
    "                  bbox_to_anchor=(0.5, -0.2))\n",
    "#                   bbox_to_anchor=(1.0, 1.0))\n",
    "for handle in lgnd.legendHandles:\n",
    "    handle.set_linewidth(12)\n",
    "\n",
    "plt.title(f'Carbon Regimes of {_year}, {_month} with delta_dist = {delta_dist}, delta_var={delta_var}',fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658682a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f714c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data.to_pickle(f\"output_files/clusters_spatial_regression_2009_jan.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15cec3d",
   "metadata": {},
   "source": [
    "# Analysis for Jan 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df.groupby(\"cluster\")['slope_sst_std', 'slope_dicp_std', 'slope_alk_std'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c30bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_mean = hc_df.groupby(\"cluster\")['slope_sst_std', 'slope_dicp_std', 'slope_alk_std'].mean().reset_index()\n",
    "regime_mean.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34078c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_mean = hc_df.groupby(\"cluster\")['slope_sst_std', 'slope_dicp_std', 'slope_alk_std'].mean().reset_index()\n",
    "regime_mean.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_var = hc_df.groupby(\"cluster\")['slope_sst_std', 'slope_dicp_std', 'slope_alk_std'].var().reset_index()\n",
    "regime_var.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_mean['slope_sst_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47cb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regime_names = ['ICE II', 'ICE I', 'SUBTR II', 'SUBTR I', 'SUBP + UP II', 'ICE III', 'SUBP + UP I']\n",
    "regime_names_dict = {\n",
    "    1: 'ICE I',\n",
    "    2: 'ICE II',\n",
    "    3: 'SUBTR I',\n",
    "    4: 'SUBTR II',\n",
    "    6: 'SUBP + UP I',\n",
    "    7: 'SUBP + UP II',\n",
    "    5: 'SUBP + UP III',\n",
    "}\n",
    "len(regime_names_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_column_values_by_regime_name(regime_mean, col_name):\n",
    "    new_order = []\n",
    "    new_cluster_order = []\n",
    "    new_keys = list(regime_names_dict.keys())\n",
    "    new_keys.reverse()\n",
    "    for key in new_keys:\n",
    "        new_order.append(regime_mean.loc[regime_mean['cluster'] == key][col_name].values[0])\n",
    "        new_cluster_order.append(key)\n",
    "    return new_order, new_cluster_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7532a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_column_values_by_regime_name(regime_mean, 'slope_alk_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grouped horizontal bar chart for each cluster\n",
    "fig, ax = plt.subplots(figsize=(10, 6),) #dpi=1200\n",
    "# ax.grid(color='grey', alpha=0.2)\n",
    "ax.grid(zorder=0)\n",
    "\n",
    "# Get number of clusters\n",
    "num_clusters = len(regime_mean)\n",
    "\n",
    "# Set width for each bar group\n",
    "bar_width = 0.2\n",
    "\n",
    "# Set positions for each cluster starting from 1\n",
    "positions = np.arange(1, num_clusters + 1)\n",
    "\n",
    "# Plot bars for each column\n",
    "for i, col in enumerate(regime_mean.columns[1:]):  # Exclude \"Cluster ID\" column\n",
    "    if col == 'slope_dicp_std':\n",
    "        lbl = 'DIC'\n",
    "    else:\n",
    "        lbl = col.split(\"_\")[1].upper()\n",
    "    # Determine position for the bar group\n",
    "    pos = positions + i * bar_width - bar_width * (len(regime_mean.columns[1:]) - 1) / 2\n",
    "#     print(col)\n",
    "#     print(pos)\n",
    "#     print()\n",
    "    \n",
    "    # Plot bars\n",
    "    values, new_cluster_order = order_column_values_by_regime_name(regime_mean, col)\n",
    "    if col == 'slope_sst_std':\n",
    "        ax.barh(pos, values, bar_width, label=lbl, fill=False, hatch='|', zorder=3)\n",
    "    if col == 'slope_dicp_std':\n",
    "        ax.barh(pos, values, bar_width, label=lbl, fill=False, hatch='..', zorder=3)\n",
    "    if col == 'slope_alk_std':\n",
    "        ax.barh(pos, values, bar_width, label=lbl, fill=False, hatch='///', zorder=3)\n",
    "\n",
    "\n",
    "# Add labels, legend, and title\n",
    "ax.set_yticks(positions)\n",
    "## Update the cluster labels\n",
    "# ax.set_yticklabels(regime_mean.cluster)\n",
    "y_labels = list(regime_names_dict.values())\n",
    "y_labels.reverse()\n",
    "ax.set_yticklabels(y_labels, weight='bold')\n",
    "\n",
    "for ytick, cluster_num in zip(ax.get_yticklabels(), new_cluster_order):\n",
    "    ytick.set_color(cluster_color_dict[cluster_num])\n",
    "\n",
    "legend_properties = {'weight':'bold', 'size':16}\n",
    "ax.legend(loc='lower right', prop=legend_properties)\n",
    "\n",
    "ax.set_xlabel('Mean normalized RCs', fontsize=24)\n",
    "ax.set_ylabel('Carbon regimes', fontsize=24)\n",
    "# ax.set_title('Mean regression coeffcients of drivers for each carbon provinces', fontsize=15)\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['xtick.labelsize'] = 20\n",
    "mpl.rcParams['ytick.labelsize'] = 20\n",
    "\n",
    "# Show plot\n",
    "plt.savefig(f\"output_tracking/figs/cluster_details_2009_jan\",\n",
    "            dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363fbfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grouped horizontal bar chart for each cluster\n",
    "fig, ax = plt.subplots(figsize=(10, 6),) #dpi=1200\n",
    "# ax.grid(color='grey', alpha=0.2)\n",
    "ax.grid(zorder=0)\n",
    "\n",
    "# Get number of clusters\n",
    "num_clusters = len(regime_mean)\n",
    "\n",
    "# Set width for each bar group\n",
    "bar_width = 0.2\n",
    "\n",
    "# Set positions for each cluster starting from 1\n",
    "positions = np.arange(1, num_clusters + 1)\n",
    "\n",
    "# Plot bars for each column\n",
    "for i, col in enumerate(regime_mean.columns[1:]):  # Exclude \"Cluster ID\" column\n",
    "    if col == 'slope_dicp_std':\n",
    "        lbl = 'DIC'\n",
    "    else:\n",
    "        lbl = col.split(\"_\")[1].upper()\n",
    "    # Determine position for the bar group\n",
    "    pos = positions + i * bar_width - bar_width * (len(regime_mean.columns[1:]) - 1) / 2\n",
    "#     print(col)\n",
    "#     print(pos)\n",
    "#     print()\n",
    "    \n",
    "    # Plot bars\n",
    "    if col == 'slope_sst_std':\n",
    "        ax.barh(pos, regime_mean[col], bar_width, label=lbl, fill=False, hatch='|', zorder=3)\n",
    "    if col == 'slope_dicp_std':\n",
    "        ax.barh(pos, regime_mean[col], bar_width, label=lbl, fill=False, hatch='..', zorder=3)\n",
    "    if col == 'slope_alk_std':\n",
    "        ax.barh(pos, regime_mean[col], bar_width, label=lbl, fill=False, hatch='///', zorder=3)\n",
    "\n",
    "\n",
    "# Add labels, legend, and title\n",
    "ax.set_yticks(positions)\n",
    "## Update the cluster labels\n",
    "# ax.set_yticklabels(regime_mean.cluster)\n",
    "ax.set_yticklabels(regime_names)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Mean normalized RCs', fontsize=20)\n",
    "ax.set_ylabel('Carbon uptake provinces', fontsize=20)\n",
    "# ax.set_title('Mean regression coeffcients of drivers for each carbon provinces', fontsize=15)\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['xtick.labelsize'] = 20\n",
    "mpl.rcParams['ytick.labelsize'] = 20\n",
    "\n",
    "# Show plot\n",
    "plt.savefig(f\"output_tracking/figs/cluster_details_2009_jan\",\n",
    "            dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dend-proj",
   "language": "python",
   "name": "dend_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "192px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
